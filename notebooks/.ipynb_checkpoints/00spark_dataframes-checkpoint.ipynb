{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframes - Basic Operations and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (IntegerType, StringType, StructField, StructType)\n",
    "from pyspark.sql.functions import (countDistinct, max, mean, stddev, format_number)\n",
    "from pyspark.sql.functions import (month, year, hour, dayofmonth, dayofyear, date_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the spark session and creating a Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spark_app_1').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in a `JSON` file into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatically reading in a file - Spark assigns its own schema to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_data_file = os.getcwd() + '/data/people.json'\n",
    "people_df = spark.read.json(people_data_file)\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually assigning a schema to the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_schema = [StructField('age', IntegerType(), True),\n",
    "              StructField('name', StringType(), True)] \n",
    "\n",
    "##StructField (column-name, what type it is, whether field can be Null)\n",
    "## data_schema defines the schema\n",
    "\n",
    "final_struc = StructType(fields = data_schema)\n",
    "people_df = spark.read.json(people_data_file, schema = final_struc)\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the structure of a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.printSchema() ##Function that prints the structure/schema of a DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the column names of a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.columns ##Attribute of a dataframe, therefore, no ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run summary statistics on a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = people_df.describe() ## Returns a dataframe that can be stored in a variable\n",
    "summary.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data from dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'age'>\n",
      "DataFrame[age: int]\n",
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Getting a column\n",
    "sub = people_df['age'] ## this returns an object of type pyspark.sql.column.Column = just a column\n",
    "print(sub)\n",
    "sub = people_df.select('age') ## This selects the age column from the data from a column but returns a dataframe\n",
    "print(sub)\n",
    "sub.show()\n",
    "subset = people_df.select(['age', 'name'])\n",
    "subset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "top_2 = people_df.head(2)\n",
    "print(top_2) ## i.e, head() returns a list of Row objects\n",
    "\n",
    "## To get values from a row\n",
    "print(top_2[1]['age']) ## i.e, get row index 0,1... and column name => hybrid indexing numeric and key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating/adding new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumn('double_age', people_df['age'] * 2) ## withColumn(new col name, transformation on existing col)\n",
    "##withColumn returns a new df, the original DF remains unchanged\n",
    "new_df = people_df.withColumn('double_age', people_df['age'] * 2)\n",
    "new_df.show()\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumnRenamed('age', 'customer_age') ## Inplace operation of renaming a col, withColumnRenamed(old name, new name)\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new view, i.e, turn the DF to a table\n",
    "people_df.createOrReplaceTempView('people') ## Creates a table called people and registers it as a Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Andy|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = spark.sql(\"SELECT name FROM people WHERE age > 25\") ## SQL Query is the param, returns DF\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations on a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_file = os.getcwd() + '/data/appl_stock.csv'\n",
    "stock_df = spark.read.csv(stock_file, inferSchema = True, header = True)\n",
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11 00:00:00|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12 00:00:00|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13 00:00:00|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14 00:00:00|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15 00:00:00|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20 00:00:00|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21 00:00:00|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27 00:00:00|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|               Date|              Open|             Close|\n",
      "+-------------------+------------------+------------------+\n",
      "|2012-02-13 00:00:00|        499.529991|502.60002099999997|\n",
      "|2012-02-14 00:00:00|        504.659988|        509.459991|\n",
      "|2012-02-16 00:00:00|        491.500008|502.20999900000004|\n",
      "|2012-02-17 00:00:00|        503.109993|         502.12001|\n",
      "|2012-02-21 00:00:00|506.88001299999996|        514.850021|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+------------------+------------------+\n",
      "|               Date|              Open|             Close|\n",
      "+-------------------+------------------+------------------+\n",
      "|2012-02-13 00:00:00|        499.529991|502.60002099999997|\n",
      "|2012-02-14 00:00:00|        504.659988|        509.459991|\n",
      "|2012-02-16 00:00:00|        491.500008|502.20999900000004|\n",
      "|2012-02-17 00:00:00|        503.109993|         502.12001|\n",
      "|2012-02-21 00:00:00|506.88001299999996|        514.850021|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+------------------+------------------+\n",
      "|               Date|              Open|             Close|\n",
      "+-------------------+------------------+------------------+\n",
      "|2012-02-13 00:00:00|        499.529991|502.60002099999997|\n",
      "|2012-02-16 00:00:00|        491.500008|502.20999900000004|\n",
      "|2013-01-16 00:00:00|494.63999900000005|506.08998099999997|\n",
      "|2013-01-18 00:00:00|        498.519981|        500.000015|\n",
      "|2013-10-17 00:00:00|        499.979988|        504.499985|\n",
      "|2014-01-31 00:00:00|        495.179985|500.59997599999997|\n",
      "+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub = stock_df.filter(\"Close > 500\").select(['Date', 'Open', 'Close']) ## Filter takes in a SQL where clause statement\n",
    "## Select can be applied after filter to prune the data set further\n",
    "sub.show(5)\n",
    "\n",
    "## Similarly ...\n",
    "sub = stock_df.filter(stock_df['Close'] > 500).select(['Date', 'Open', 'Close']) ## Filter also takes python conditions\n",
    "sub.show(5)\n",
    "\n",
    "## Multiple conditions in filter\n",
    "sub = stock_df.filter((stock_df['Open'] < 500) & (stock_df['Close'] > 500)).select(['Date', 'Open', 'Close']) ## Each condition muct be wrapped in ()\n",
    "sub.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `collect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'list'>\n",
      "[Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]\n",
      "Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)\n",
      "2010-01-22 00:00:00 220441900\n"
     ]
    }
   ],
   "source": [
    "res = stock_df.filter(stock_df['Low'] == 197.16) ## Returns DF for 1 row\n",
    "print(type(res))\n",
    "\n",
    "res = stock_df.filter(stock_df['Low'] == 197.16).collect() ## Collect returns the data as a list of Row datatypes\n",
    "print(type(res)) ## List\n",
    "print(res) ## List containing Rows\n",
    "print(res[0]) ## First Row\n",
    "\n",
    "row_dict = res[0].asDict() ## Can be converted to a dict\n",
    "print(row_dict['Date'], row_dict['Volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrames GroupBy & Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_file = os.getcwd() + '/data/sales_info.csv'\n",
    "sales_df = spark.read.csv(sales_file, inferSchema = True, header = True)\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.group.GroupedData object at 0x7ff09047ef28>\n",
      "DataFrame[Company: string, avg(Sales): double]\n",
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sales_df.groupBy(\"Company\")) ## GroupedData object\n",
    "print(sales_df.groupBy(\"Company\").mean()) ## Returns DF\n",
    "sales_df.groupBy(\"Company\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "company_mean = sales_df.groupBy(\"Company\").mean() ## Different functions: min, max, sum, count\n",
    "company_mean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.agg({'Sales' : 'sum'}).show() ## in agg({variable/column : function}), returns DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data = sales_df.groupBy('Company')\n",
    "max_sales_per_company = group_data.agg({'Sales' : 'max'})\n",
    "max_sales_per_company.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|Company|max_sales|\n",
      "+-------+---------+\n",
      "|   APPL|    750.0|\n",
      "|   GOOG|    340.0|\n",
      "|     FB|    870.0|\n",
      "|   MSFT|    600.0|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Using agg returns a new DF with columns named by the operation performed, \n",
    "## we can provide our own name to that col\n",
    "max_sales_per_company = group_data.agg(max('Sales').alias('max_sales'))\n",
    "max_sales_per_company.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|Company|stdev_sales|\n",
      "+-------+-----------+\n",
      "|   APPL|     268.82|\n",
      "|   GOOG|     111.36|\n",
      "|     FB|     367.70|\n",
      "|   MSFT|     247.72|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sd_sales_per_company = group_data.agg(stddev('Sales').alias('stdev_sales'))\n",
    "sd_sales_per_company = sd_sales_per_company.select(['Company',\n",
    "                                                    format_number('stdev_sales', 2).alias('stdev_sales')])\n",
    "sd_sales_per_company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordering the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asc = sales_df.orderBy('Sales') ## Sorting/Ordering the DF in ascending order\n",
    "asc.show()\n",
    "desc = sales_df.orderBy(sales_df['Sales'].desc()) ## Sorting/Ordering the DF in ascending order, note: column has to be passed instead of column name\n",
    "desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data in Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_file = os.getcwd() + '/data/ContainsNull.csv'\n",
    "df = spark.read.csv(df_file, inferSchema = True, header = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all `null`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show() ## Removes all rows that contain any missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `thresh` to remove some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh = 2).show() ## Returns only rows that have greater than threshold non-null entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `how`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = 'all').show() ##Drops only rows where all entities are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = 'any').show() ##Drops rows where any entity is null, i.e same as df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `subset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset = ['Sales']).show() ##Drops rows where Sales is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling in Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Consider the DF schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2|  val| null|\n",
      "|emp3|  val|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 2 string cols and 1 numeric col\n",
    "df.na.fill(\"val\").show() ## Spark is able to resolve the datatype of the param to the datatype of the col\n",
    "## i.e, if fill has string value, nulls in string columns will be assigned that value\n",
    "## similarly ...\n",
    "df.na.fill(0).show()\n",
    "## if fill has numeric value, nulls in numeric columns will be assigned that value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We can also embark on a column centric approach\n",
    "df.na.fill(0, subset = ['Sales']).show() ## Fill 0 for nulls in this subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Sales)=400.5)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can also use standard DS conventions to handle missing values, \n",
    "## for example: fill in Nulls using the mean (numeric columns)\n",
    "\n",
    "df_sales_mean = df.select(mean('Sales')).collect()\n",
    "df_sales_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_sales = df_sales_mean[0][0] ##gets the numeric value from the list\n",
    "df.na.fill(mean_sales, subset = 'Sales').show()\n",
    "\n",
    "## Everything can also be squished into one line as follows\n",
    "df.na.fill(df.select(mean('Sales')).collect()[0][0],\n",
    "          ['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Dates and Timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `datetime` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2010, 1, 4, 0, 0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_sample = stock_df.head(1)[0].asDict()['Date']\n",
    "date_sample ## Datetime Object (Y, M, D, HR, MIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark functions for handling `datetime` elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+-----+----+\n",
      "|             Close|day|month|year|\n",
      "+------------------+---+-----+----+\n",
      "|            197.75| 22|    1|2010|\n",
      "|        199.289995| 28|    1|2010|\n",
      "|        192.060003| 29|    1|2010|\n",
      "|        194.729998|  1|    2|2010|\n",
      "|        195.859997|  2|    2|2010|\n",
      "|        199.229994|  3|    2|2010|\n",
      "|        192.050003|  4|    2|2010|\n",
      "|        195.460001|  5|    2|2010|\n",
      "|194.11999699999998|  8|    2|2010|\n",
      "|196.19000400000002|  9|    2|2010|\n",
      "|195.12000700000002| 10|    2|2010|\n",
      "|        198.669994| 11|    2|2010|\n",
      "|        197.059998| 23|    2|2010|\n",
      "|         93.699997|  9|    6|2014|\n",
      "|             94.25| 10|    6|2014|\n",
      "|         93.860001| 11|    6|2014|\n",
      "|         92.290001| 12|    6|2014|\n",
      "|         91.279999| 13|    6|2014|\n",
      "|         92.199997| 16|    6|2014|\n",
      "| 92.08000200000001| 17|    6|2014|\n",
      "+------------------+---+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We can use Spark functions to access the elements of the datetime functions as per our needs\n",
    "## For example, we can find the days of the month, month and years when the closing price was less than 200\n",
    "sub = stock_df.filter(stock_df['Close'] < 200).select(['Date', 'Open', 'Close'])\n",
    "sub.select(['Close', dayofmonth(sub['Date']).alias('day'),\n",
    "            month(sub['Date']).alias('month'), year(sub['Date']).alias('year')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----+\n",
      "|               Date|             Close|Year|\n",
      "+-------------------+------------------+----+\n",
      "|2010-01-04 00:00:00|        214.009998|2010|\n",
      "|2010-01-05 00:00:00|        214.379993|2010|\n",
      "|2010-01-06 00:00:00|        210.969995|2010|\n",
      "|2010-01-07 00:00:00|            210.58|2010|\n",
      "|2010-01-08 00:00:00|211.98000499999998|2010|\n",
      "|2010-01-11 00:00:00|210.11000299999998|2010|\n",
      "|2010-01-12 00:00:00|        207.720001|2010|\n",
      "|2010-01-13 00:00:00|        210.650002|2010|\n",
      "|2010-01-14 00:00:00|            209.43|2010|\n",
      "|2010-01-15 00:00:00|            205.93|2010|\n",
      "|2010-01-19 00:00:00|        215.039995|2010|\n",
      "|2010-01-20 00:00:00|            211.73|2010|\n",
      "|2010-01-21 00:00:00|        208.069996|2010|\n",
      "|2010-01-22 00:00:00|            197.75|2010|\n",
      "|2010-01-25 00:00:00|        203.070002|2010|\n",
      "|2010-01-26 00:00:00|        205.940001|2010|\n",
      "|2010-01-27 00:00:00|        207.880005|2010|\n",
      "|2010-01-28 00:00:00|        199.289995|2010|\n",
      "|2010-01-29 00:00:00|        192.060003|2010|\n",
      "|2010-02-01 00:00:00|        194.729998|2010|\n",
      "+-------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---------+\n",
      "|Year|avg_close|\n",
      "+----+---------+\n",
      "|2010|   259.84|\n",
      "|2011|   364.00|\n",
      "|2012|   576.05|\n",
      "|2013|   472.63|\n",
      "|2014|   295.40|\n",
      "|2015|   120.04|\n",
      "|2016|   104.60|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We can use datetime functions in more advanced queries such as : \n",
    "## Average Close price by Year\n",
    "new_df = stock_df.select(['Date', 'Close'])\n",
    "new_df = new_df.withColumn('Year', year(new_df['Date']))\n",
    "new_df.show()\n",
    "avg_year_close = new_df.groupBy('Year').mean('Close')\n",
    "avg_year_close = avg_year_close.select(['Year', format_number('avg(Close)', 2).alias('avg_close')])\n",
    "avg_year_close.orderBy('Year').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
